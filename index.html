<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="DADA: Dialect Adaptation via Dynamic Aggregation of Linguistic Rules">
  <meta name="keywords" content="Dialect, Dialect Adaptation, Linguistic Diversity, Fairness, Human-Centered NLP">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>DADA: Dialect Adaptation via Dynamic Aggregation of Linguistic Rules</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/panda.png">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
  <style>
    .AppE {
        color: #DAA520; /* myyellow */
        font-family: monospace;
        font-weight: bold;
    }
    .ChcE {
        color: #6F42C1; /* mypurple */
        font-family: monospace;
        font-weight: bold;
    }
    .CollSgE {
        color: #D55E00; /* myorange */
        font-family: monospace;
        font-weight: bold;
    }
    .IndE {
        color: #556B2F; /* myolivegreen */
        font-family: monospace;
        font-weight: bold;
    }
    .AAVE {
        color: #990000; /* myred */
        font-family: monospace;
        font-weight: bold;
    }
    .SAE {
        color: #2F5596; /* mydarkblue */
        font-family: monospace;
        font-weight: bold;
    }
    .highlight {
        background-color: #F9CD69;
        font-weight: bold;
    }
  </style>

<body>
  
  

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">DADA: Dialect Adaptation via Dynamic Aggregation of Linguistic Rules</h1>

          <h3 class="title is-3" style="color: #990000;">Dialect Adaptation from a Fine-Grained Level</h3>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://yanchenliu1015.github.io/">Yanchen Liu</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://williamheld.com/">William Held</a><sup>2</sup>,</span>
            <span class="author-block">
              <a href="https://cs.stanford.edu/~diyiy/">Diyi Yang</a><sup>3</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Harvard University,</span>
            <span class="author-block"><sup>2</sup>Georgia Institute of Technology,</span>
            <span class="author-block"><sup>3</sup>Stanford University</span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <img src="./static/images/harvard-university-logo.png" width="250" align="absmiddle"/>  
            </span>
            <span class="author-block">
              <img src="./static/images/GeorgiaTech_RGB.png" width="200" align="absmiddle"/>  
            <span class="author-block">
              <img src="./static/images/stanford-university-logo-2.png" width="200" align="absmiddle"/>  
            </span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/pdf/2305.13406.pdf"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/abs/2305.13406"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/SALT-NLP/DADA"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
<!--               <span class="link-block">
                <a href="https://github.com/SALT-NLP/DADA_data"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span> -->
                  </a>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser" style="margin-bottom: 0;">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <figure class="image-container" style="text-align: center;">
        <img src="./static/images/DADA.png"
             class="summary-image"
             alt="DADA"
             style="width: 60%; display: block; margin-left: auto; margin-right: auto;">
        <figcaption class="image-caption" style="margin-top: 8px; margin-bottom: 0px; font-size: 0.9em;">
          DADA dynamically composes adapters that handle specific features of dialectal variation to adapt an <span class="SAE">SAE</span> model to various dialects by leveraging their commonality. We train nearly 200 feature adapters to capture the linguistic differences between <span class="SAE">SAE</span> and its dialect variants. These feature adapters can be composed flexibly and arbitrarily to target different dialects.
        </figcaption>
      </figure>
    </div>
  </div>
</section>
           
<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Existing large language models (LLMs) that mainly focus on Standard American English (<span class="SAE">SAE</span>) often lead to significantly worse performance when being applied to other English dialects. While existing mitigations tackle discrepancies for individual target dialects, they assume access to high-accuracy dialect identification systems. The boundaries between dialects are inherently flexible, making it difficult to categorize language into discrete predefined categories. In this paper, we propose DADA (Dialect Adaptation via Dynamic Aggregation), a modular approach to imbue <span class="SAE">SAE</span>-trained models with multi-dialectal robustness by composing adapters which handle specific linguistic features. The compositional architecture of DADA allows for both targeted adaptation to specific dialect variants and simultaneous adaptation to various dialects. We show that DADA is effective for both single task and instruction finetuned language models, offering an extensible and interpretable framework for adapting existing LLMs to different English dialects.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
</section>


<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column">
        <div class="content">
          <h2 class="title is-3">Background</h2>
          <p>
            Current NLP tooling is often trained and evaluated on dominant language variants, such as Standard American English (<span class="SAE">SAE</span>). This results in a significant decline in performance when these tools are applied to non-<span class="SAE">SAE</span> dialects (even LLMs are not exempt). Such performance disparities raise ethical and moral concerns regarding the potential for racial disparities in the seemingly expeditious development of language technologies.
          </p>

          <div class="image-section" style="text-align: center;">
            <img src="./static/images/dialect_drop.png" style="max-width: 50%; height: auto; display: block; margin-left: auto; margin-right: auto;">
          </div>

          <p>
            Existing research to mitigate this disparity has mainly focused on dialectal adaptation targeting individual dialects of interest. These methods require the development of separate systems for different dialects and highly accurate dialect identification systems for real-world uses. However, such separate systems are not yet available for many dialects and related languages.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column">
        <div class="content">
          <h2 class="title is-3">Dialect and Non-Standard Linguistic Feature</h2>
          <p>
            Dialects can be described by a common set of linguistic rules (non-standard linguistic features), that describe the differences between <span class="SAE">SAE</span> and various other English dialects. Each dialect can express a subset of this feature space. In addition, dialects are not deterministic patterns but rather ranges of acceptable use of these features that speakers adjust based on social contexts. Therefore, dialects do not neatly fit into predefined categories.
            Here is an example: Negative Concord, a linguistic feature common in many English dialects, involves using multiple negative words or phrases to reinforce a negative meaning.
          </p>

          <div class="image-section" style="text-align: center;">
            <img src="./static/images/example.png" alt="feature_example" style="max-width: 50%; height: auto; display: block; margin-left: auto; margin-right: auto; margin-bottom: 20px;">
          </div>

          <p>
            To this end, our work proposes a method that can accommodate the diversity of dialect variants at a <span class="highlight">fine-grained level</span> (non-standard linguistic features or linguistic rules).
          </p>
          
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column">
        <div class="content">
          <h2 class="title is-3">Dialect Adaptation via Dynamic Aggregation</h2>
          <p>
            We introduce Dialect Adaptation via Dynamic Aggregation (DADA), a modular method for adapting an existing model trained on the <span class="SAE">SAE</span> to accommodate dialect variants at a finer-grained level, in 3 steps.
          </p>

          <div class="image-section" style="text-align: center;">
            <img src="./static/images/process.png" alt="process" style="max-width: 90%; height: auto; display: block; margin-left: auto; margin-right: auto; margin-bottom: 50px;">
          </div>
          <h4 class="title is-5">Step 1: Synthetic Datasets Construction</h4>
          <p>
            Previous works have discerned a series of linguistic divergences and devised <a href="https://value-nlp.org/">Multi-Value</a>, a collection of lexical and morphosyntactic transformation rules between <span class="SAE">SAE</span> and its 50 dialect variants, including Appalachian English (<span class="AppE">AppE</span>), Chicano English (<span class="ChcE">ChcE</span>), Colloquial Singapore English (<span class="CollSgE">CollSgE</span>), Indian English (<span class="IndE">IndE</span>), and African American Vernacular English (<span class="AAVE">AAVE</span>), among others. For each transformation rule, we first generate a corresponding synthetic dataset by applying the respective rule to each individual training example within the original training dataset.
          </p>

          <h4 class="title is-5" style="margin-top: 30px;">Step 2: Feature Adapter Training</h4>
          <p>
            Secondly, we develop a feature adapter for each linguistic transformation rule by training it on the corresponding synthetic dataset. Each trained feature adapter can capture a specific type of linguistic difference between <span class="SAE">SAE</span> and its dialect variants.
          </p>

          <h4 class="title is-5" style="margin-top: 30px;">Step 3: Dynamic Aggregation</h4>
          <p>
            However, it is common for multiple linguistic differences to co-occur within a single sentence in real-world scenarios, thereby necessitating the model to consider these distinct linguistic features to varying degrees simultaneously.
          </p>
          <p>
            Therefore, in the third step, we propose to dynamically aggregate the trained feature adapters, into the <span class="SAE">SAE</span>-trained backbone model via an additional fusion layer.
            Through training on the super-synthetic dataset, a parameterized compositional mixture of feature adapters can be learned to identify the applied linguistic features for a given input and activate the corresponding feature adapters, thereby facilitating the effective addressing of linguistic discrepancies between <span class="SAE">SAE</span> and its dialect variants.
          </p>
          
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column">
        <div class="content">
          <h2 class="title is-3">DADA Can Improve Multi-Dialectal Robustness!</h2>
          <p>
            We demonstrate here how DADA can enable the adaptation of an existing <span class="SAE">SAE</span> model to multiple dialect variants, taking the RoBERTa Base model and MNLI task as an example. 
          </p>

          <div class="image-section" style="text-align: center;">
            <img src="./static/images/multi_dialect.png" alt="process" style="max-width: 80%; height: auto; display: block; margin-left: auto; margin-right: auto; margin-bottom: 50px;">
          </div>

          <p>
            Compared to the standard <span class="SAE">SAE</span> model trained on the original MNLI dataset (Baseline), DADA demonstrates significant performance improvements across all evaluated dialects and even on <span class="SAE">SAE</span>. 
            Moreover, DADA delivers comparable performance to the strong baseline provided by individual further adapter tuning on the <span class="SAE">SAE</span>-trained model with dialect-specific training data (Individual).
            However, while this approach requires a perfect dialect identification system and multiple models, our approach uses a single model and therefore does not rely on dialect identification.
            This makes DADA a simpler and more realistic method for use when the target dialect distribution is unknown.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column">
        <div class="content">
          <h2 class="title is-3">DADA Can Be Task-Agnostic!</h2>
          <p>
            Recent LLMs are instruction-tuned for various tasks, which is orthogonal to our method, making it possible to combine the two approaches easily. Here, we demonstrate that DADA can be employed in instruction-tuned LLMs to improve their task-agnostic performance on dialects!
          </p>
          
          <div class="image-section" style="text-align: center;">
            <img src="./static/images/multi_task.png" alt="process" style="max-width: 80%; height: auto; display: block; margin-left: auto; margin-right: auto; margin-bottom: 50px;">
          </div>

          <p>
            It is surprising to note that although individual adapter tuning (Individual) demonstrates improvements in 4 out of 7 tasks, the overall average performance is even inferior to that of the <span class="SAE">SAE</span> baseline.
            In contrast, DADA consistently outperforms both the <span class="SAE">SAE</span> Baseline and Individual across all evaluated tasks.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column">
        <div class="content">
          <h2 class="title is-3">DADA Has Great Interpretability!</h2>
          <p>
            We perform a correlation analysis of the 10 feature adapters of <span class="AAVE">AAVE</span> for the linguistic features applied to the input data. We plot the results for layers 1, 3, 7, 11 here.
          </p>
          
          <figure class="image-container" style="text-align: center;">
            <img src="./static/images/coefplots.png" alt="process" style="max-width: 100%; height: auto; display: block; margin-left: auto; margin-right: auto; margin-bottom: 50px;">
            <figcaption class="image-caption" style="margin-top: 8px; margin-bottom: 0px; font-size: 0.9em;">
              Correlation Coefficients for <span class="AAVE">AAVE</span> adaptation between the feature adapters (column) and the inputs to which specific linguistic features (row) apply in layers 1, 3, 7, 11. We use abbreviations for certain terms, such as "nc" for "negative_concord."
            </figcaption>
          </figure>
          
          <p>
            We find that significant correlations in utilization on the lower layers (0-3) can be observed, while those on the middle and higher layers are found to be negligible.
            This is consistent with intuition, as the primary distinction between <span class="SAE">SAE</span> and its dialect variants lies in their linguistic features (lexical and morphosyntactic), which are mainly captured by the lower layers of the model.
            This analysis demonstrates that DADA can detect which linguistic features are relevant to the given input, and subsequently trigger the corresponding feature adapters. This highlights the interpretability of DADA!
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column">
        <div class="content">
          <h2 class="title is-3">Ethics Statement</h2>
          <p>
            Previous linguistic works on dialectal features may not fully or accurately document the natural usage patterns of all existing dialects in terms of their linguistic rules.
            As a result, we acknowledge that our proposed method, which relies on these dialectal features from prior literature, may not take some undocumented features associated with dialects into account. However, by curating more dialectal features, our method can be easily extended to a broader range of dialects.
            Additionally, as DADA is task-agnostic when applied to instruction-tuned models, malicious individuals might misuse it. To address this concern, we release DADA with a license that explicitly prohibits its usage for purposes of deception, impersonation, mockery, discrimination, hate speech, targeted harassment, and cultural appropriation targeting dialect-speaking communities.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column">
        <div class="content">
          <h2 class="title is-3">Acknowledgement</h2>
          <p>
            We would like to thank the anonymous reviewers and SALT lab members for their valuable feedback! This work was partially sponsored by the Defense Advanced Research Project Agency (DARPA) and NSF grant.</p>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>
      @inproceedings{liu2023dada,
         title={DADA: Dialect Adaptation via Dynamic Aggregation of Linguistic Rules},
         author={Yanchen Liu and William Held and Diyi Yang},
         booktitle={Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing},
         year={2023}
      }
    </code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            This source code of this website is borrowed from <a
              href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
